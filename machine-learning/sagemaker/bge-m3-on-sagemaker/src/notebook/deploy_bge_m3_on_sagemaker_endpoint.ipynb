{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3b0a4b-f166-4f1a-a7cc-9c7277c68173",
   "metadata": {},
   "source": [
    "# Deploying BGE-M3 Embedding Model on Amazon SageMaker\n",
    "\n",
    "This notebook demonstrates how to deploy the [BGE-M3](https://huggingface.co/BAAI/bge-m3) embedding model on Amazon SageMaker. BGE-M3 is a state-of-the-art embedding model that supports dense, sparse, and ColBERT embeddings.\n",
    "\n",
    "## Steps:\n",
    "1. Download model checkpoint from Hugging Face\n",
    "2. Upload model to S3\n",
    "3. Create custom inference code\n",
    "4. Deploy model to SageMaker endpoint\n",
    "5. Test the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b61ad8-a8c2-48c2-8539-e7c1e2afe773",
   "metadata": {},
   "source": [
    "## 1. Download Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be112a00-cbef-4387-b0d7-80e5e7b7030d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_model_path = Path(\"./hf_model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "snapshot_download(repo_id=model_name, cache_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b61ad8-a8c2-48c2-8539-e7c1e2afe774",
   "metadata": {},
   "source": [
    "## 2. Upload Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e177a-886d-4838-891e-2e612a3cbc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Initialize SageMaker session and clients\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Define S3 paths\n",
    "s3_model_prefix = \"BAAI/hf_model\"  # folder where model checkpoint will go\n",
    "model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "s3_code_prefix = \"BAAI/inference_code\"\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")\n",
    "\n",
    "# Upload model to S3\n",
    "!aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f35a6f-5988-42ec-87b0-de36eaebe41b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Create Custom Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159677b1-2cbd-4ca1-8cd4-063a6f1c8a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p inference_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86daea77-a7ae-46b8-8800-212d07ce5605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile inference_code/model.py\n",
    "from djl_python import Input, Output\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'--device={device}')\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties.get(\"tensor_parallel_degree\", 1)  # Default value 1\n",
    "    model_location = properties.get(\"model_dir\", \"/opt/ml/model\")\n",
    "    \n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties[\"model_id\"]\n",
    "    \n",
    "    logging.info(f\"Loading model from {model_location}\")\n",
    "\n",
    "    model = BGEM3FlagModel(model_location, use_fp16=True)\n",
    "    return model\n",
    "\n",
    "model = None\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model\n",
    "    if model is None:\n",
    "        model = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_json()\n",
    "\n",
    "    # Extract parameters from JSON\n",
    "    input_sentences = data.get(\"inputs\", [])\n",
    "    if isinstance(input_sentences, str):\n",
    "        input_sentences = [input_sentences]  # Convert single input to list\n",
    "\n",
    "    is_query = data.get(\"is_query\", False)\n",
    "    max_length = data.get(\"max_length\", 2048)\n",
    "    instruction = data.get(\"instruction\", \"\")\n",
    "\n",
    "    # Extract optional parameters\n",
    "    return_dense = data.get(\"return_dense\", True)  # Default: True\n",
    "    return_sparse = data.get(\"return_sparse\", False)  # Default: False\n",
    "    return_colbert_vecs = data.get(\"return_colbert_vecs\", False)  # Default: False\n",
    "\n",
    "    logging.info(f\"inputs: {input_sentences}\")\n",
    "    logging.info(f\"is_query: {is_query}\")\n",
    "    logging.info(f\"instruction: {instruction}\")\n",
    "    logging.info(f\"return_dense: {return_dense}, return_sparse: {return_sparse}, return_colbert_vecs: {return_colbert_vecs}\")\n",
    "\n",
    "    # Add instruction for queries if provided\n",
    "    if is_query and instruction:\n",
    "        input_sentences = [instruction + sent for sent in input_sentences]\n",
    "\n",
    "    # Generate embeddings with specified options\n",
    "    sentence_embeddings = model.encode(\n",
    "        input_sentences, \n",
    "        max_length=max_length, \n",
    "        return_dense=return_dense, \n",
    "        return_sparse=return_sparse, \n",
    "        return_colbert_vecs=return_colbert_vecs\n",
    "    )\n",
    "\n",
    "    # Format output JSON\n",
    "    result = {}\n",
    "    if return_dense:\n",
    "        result[\"dense_embeddings\"] = sentence_embeddings.get(\"dense_vecs\", [])\n",
    "    if return_sparse:\n",
    "        result[\"sparse_embeddings\"] = sentence_embeddings.get(\"lexical_weights\", [])\n",
    "    if return_colbert_vecs:\n",
    "        result[\"colbert_vectors\"] = sentence_embeddings.get(\"colbert_vecs\", [])\n",
    "\n",
    "    return Output().add_as_json(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b126565-66e2-4987-ac6b-e02f09070a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"inference_code\"):\n",
    "    os.mkdir(\"inference_code\")\n",
    "\n",
    "# Create serving.properties file\n",
    "with open('inference_code/serving.properties', 'w') as f:\n",
    "    f.write(\"engine=Python\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"option.tensor_parallel_degree=1\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"option.model_id=s3://{bucket}/{s3_model_prefix}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a7806-afc4-4ae7-9253-1c9dfabfed99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile inference_code/requirements.txt\n",
    "FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe41472-c2cf-4bb5-99aa-84df76c629b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Package and upload inference code\n",
    "!rm -f inference_code.tar.gz\n",
    "!cd inference_code && rm -rf \".ipynb_checkpoints\"\n",
    "!tar czvf inference_code.tar.gz inference_code\n",
    "\n",
    "s3_code_artifact = sess.upload_data(\"inference_code.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb01ed-6bd3-4880-a647-cfd71e692820",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Deploy Model to SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbee569-ee6f-4330-a0e9-15085c0be9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "# Define the DJL inference container URI\n",
    "inference_image_uri = (f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124\")\n",
    "model_name = name_from_base(\"bge-m3\")\n",
    "\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Inference container image: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6209d24-8473-4256-93d3-02e4e144386b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CREATE MODEL\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact,\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "print(f\"Created Model: {model_arn}\")\n",
    "\n",
    "\n",
    "# CREATE ENDPOINT CONFIG\n",
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 5*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(endpoint_config_response)\n",
    "\n",
    "# CREATE ENDPOINT\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c71240-6878-4fed-bf7d-2c1cf75f4ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wait for endpoint deployment to complete\n",
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddba20e-fc18-480d-9940-ae39695ac450",
   "metadata": {},
   "source": [
    "## 5. Test the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28db25-6996-440c-b004-14f96cfd982d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_vector_by_sm_endpoint(questions, smr_client, endpoint_name):\n",
    "    \"\"\"Get embeddings from SageMaker endpoint\"\"\"\n",
    "    response_model = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": questions,\n",
    "                'return_sparse': True,\n",
    "                'return_colbert_vecs': True,\n",
    "            }\n",
    "        ),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    json_str = response_model['Body'].read().decode('utf8')\n",
    "    json_obj = json.loads(json_str)\n",
    "    return json_obj\n",
    "\n",
    "def cos_sim(vector1, vector2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_v1 = np.linalg.norm(vector1)\n",
    "    norm_v2 = np.linalg.norm(vector2)\n",
    "    cos_sim = dot_product / (norm_v1 * norm_v2)\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4f56a-092e-4a6a-a920-48550ec9f20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test dense embeddings\n",
    "text1 = \"How cute your dog is!\"\n",
    "text2 = \"Your dog is so cute.\"\n",
    "text3 = \"The mitochondria is the powerhouse of the cell.\"\n",
    "\n",
    "# Get dense embeddings and calculate similarity\n",
    "emb1, emb2, emb3 = get_vector_by_sm_endpoint([text1, text2, text3], smr_client, endpoint_name)['dense_embeddings']\n",
    "print(f\"Similarity between text1 and text2: {cos_sim(emb1, emb2)}\")\n",
    "print(f\"Similarity between text1 and text3: {cos_sim(emb1, emb3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e80bc1-7db1-4db4-9b66-2c69962fd882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sparse embeddings\n",
    "sparse_embeddings = get_vector_by_sm_endpoint([text1, text2, text3], smr_client, endpoint_name)['sparse_embeddings']\n",
    "sparse_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637ec62-e5a9-43e6-88c6-e97876e4bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ColBERT vectors\n",
    "cvec1, cvec2, cvec3 = get_vector_by_sm_endpoint([text1, text2, text3], smr_client, endpoint_name)['colbert_vectors']\n",
    "print(f\"ColBERT vector dimensions: {len(cvec1[0])}, {len(cvec2[0])}, {len(cvec3[0])}\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
