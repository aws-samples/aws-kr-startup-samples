{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3b0a4b-f166-4f1a-a7cc-9c7277c68173",
   "metadata": {},
   "source": [
    "# Deploying qwen3-embedding-0-6b Embedding Model on Amazon SageMaker\n",
    "\n",
    "This notebook demonstrates how to deploy the [qwen3-embedding-0-6b](https://huggingface.co/BAAI/qwen3-embedding-0-6b) embedding model on Amazon SageMaker. qwen3-embedding-0-6b is a state-of-the-art embedding model that supports dense, sparse, and ColBERT embeddings.\n",
    "\n",
    "## Steps:\n",
    "1. Download model checkpoint from Hugging Face\n",
    "2. Upload model to S3\n",
    "3. Create custom inference code\n",
    "4. Deploy model to SageMaker endpoint\n",
    "5. Test the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b61ad8-a8c2-48c2-8539-e7c1e2afe773",
   "metadata": {},
   "source": [
    "## 1. Download Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be112a00-cbef-4387-b0d7-80e5e7b7030d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_model_path = Path(\"./hf_model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "snapshot_download(repo_id=model_name, cache_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b61ad8-a8c2-48c2-8539-e7c1e2afe774",
   "metadata": {},
   "source": [
    "## 2. Upload Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e177a-886d-4838-891e-2e612a3cbc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize SageMaker session and clients\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Define S3 paths\n",
    "s3_model_prefix = \"model/Qwen/Qwen3-Embedding-0.6B\"  # folder where model checkpoint will go\n",
    "model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "s3_code_prefix = \"inference_code/Qwen/Qwen3-Embedding-0.6B\"\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")\n",
    "\n",
    "# Upload model to S3\n",
    "!aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f35a6f-5988-42ec-87b0-de36eaebe41b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Create Custom Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159677b1-2cbd-4ca1-8cd4-063a6f1c8a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p inference_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86daea77-a7ae-46b8-8800-212d07ce5605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile inference_code/model.py\n",
    "from djl_python import Input, from djl_python import Input, Output\n",
    "import logging\n",
    "from vllm import LLM, PoolingParams\n",
    "\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    \"\"\"Formats the query with a task-specific instruction.\"\"\"\n",
    "    if not task_description:\n",
    "        task_description = (\n",
    "            \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "        )\n",
    "    return f\"Instruct: {task_description}\\nQuery:{query}\"\n",
    "\n",
    "\n",
    "def load_model(properties):\n",
    "    model_location = properties.get(\"model_dir\", \"/opt/ml/model\")\n",
    "\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties[\"model_id\"]\n",
    "\n",
    "    logging.info(f\"Loading model from {model_location}\")\n",
    "\n",
    "    # Following the new example:\n",
    "    # - task=\"embed\" to use the embedding endpoint.\n",
    "    # - hf_overrides for matryoshka embeddings.\n",
    "    model = LLM(\n",
    "        model=model_location,\n",
    "        task=\"embed\",\n",
    "        hf_overrides={\"is_matryoshka\": True},\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = None\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model\n",
    "    if model is None:\n",
    "        model = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_json()\n",
    "\n",
    "    input_sentences = data.get(\"inputs\", [])\n",
    "    if isinstance(input_sentences, str):\n",
    "        input_sentences = [input_sentences]\n",
    "\n",
    "    # Parameters from the user's request, based on the new example\n",
    "    is_query = data.get(\"is_query\", False)\n",
    "    instruction = data.get(\n",
    "        \"instruction\"\n",
    "    )  # Can be None, get_detailed_instruct will use a default.\n",
    "    dim = data.get(\"dim\", -1)  # For matryoshka embeddings\n",
    "\n",
    "    logging.info(f\"inputs: {len(input_sentences)} sentences\")\n",
    "    logging.info(f\"is_query: {is_query}\")\n",
    "    if instruction:\n",
    "        logging.info(f\"custom instruction: {instruction}\")\n",
    "    logging.info(f\"embedding dimension: {dim if dim > 0 else 'default'}\")\n",
    "\n",
    "    if is_query:\n",
    "        # For queries, add instructions.\n",
    "        input_texts = [get_detailed_instruct(instruction, q) for q in input_sentences]\n",
    "    else:\n",
    "        # For documents, no instruction is needed.\n",
    "        input_texts = input_sentences\n",
    "\n",
    "    pooling_params = None\n",
    "    if dim > 0:\n",
    "        logging.info(f\"Using matryoshka embeddings with dimension: {dim}\")\n",
    "        pooling_params = PoolingParams(dimensions=dim)\n",
    "\n",
    "    # Get embeddings using model.embed\n",
    "    logging.info(\"Calling model.embed on vLLM...\")\n",
    "    outputs = model.embed(input_texts, pooling_params=pooling_params)\n",
    "    logging.info(\"model.embed call finished.\")\n",
    "\n",
    "    # Extract embeddings from vLLM output\n",
    "    embeddings = [o.outputs.embedding for o in outputs]\n",
    "    logging.info(f\"Extracted {len(embeddings)} embeddings.\")\n",
    "\n",
    "    # Format output\n",
    "    result = {\"dense_embeddings\": embeddings}\n",
    "    logging.info(\"Formatted result into a dictionary.\")\n",
    "\n",
    "    output_obj = Output().add_as_json(result)\n",
    "    logging.info(\"Created DJL Output object. Returning from handle function.\")\n",
    "\n",
    "    return output_obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b126565-66e2-4987-ac6b-e02f09070a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"inference_code\"):\n",
    "    os.mkdir(\"inference_code\")\n",
    "\n",
    "# Create serving.properties file\n",
    "with open('inference_code/serving.properties', 'w') as f:\n",
    "    f.write(\"engine=Python\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"option.rolling_batch=disable\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"option.model_id=s3://{bucket}/{s3_model_prefix}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a7806-afc4-4ae7-9253-1c9dfabfed99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile inference_code/requirements.txt\n",
    "vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe41472-c2cf-4bb5-99aa-84df76c629b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Package and upload inference code\n",
    "!rm -f inference_code.tar.gz\n",
    "!cd inference_code && rm -rf \".ipynb_checkpoints\"\n",
    "!tar czvf inference_code.tar.gz inference_code\n",
    "\n",
    "s3_code_artifact = sess.upload_data(\"inference_code.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb01ed-6bd3-4880-a647-cfd71e692820",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Deploy Model to SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbee569-ee6f-4330-a0e9-15085c0be9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "# Define the DJL inference container URI\n",
    "inference_image_uri = (f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128-v1.3\")\n",
    "model_name = name_from_base(\"qwen3-embedding-0-6b\")\n",
    "\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Inference container image: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6209d24-8473-4256-93d3-02e4e144386b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CREATE MODEL\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact,\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "print(f\"Created Model: {model_arn}\")\n",
    "\n",
    "\n",
    "# CREATE ENDPOINT CONFIG\n",
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 5*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(endpoint_config_response)\n",
    "\n",
    "# CREATE ENDPOINT\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c71240-6878-4fed-bf7d-2c1cf75f4ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wait for endpoint deployment to complete\n",
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddba20e-fc18-480d-9940-ae39695ac450",
   "metadata": {},
   "source": [
    "## 5. Test the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28db25-6996-440c-b004-14f96cfd982d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "# Use the endpoint name from your CDK output\n",
    "endpoint_name = \"qwen3-embedding-0-6b-endpoint\" \n",
    "\n",
    "def get_embeddings(texts, is_query=False, dim=-1):\n",
    "    \"\"\"\n",
    "    Invokes the SageMaker endpoint to get embeddings.\n",
    "    - For queries, set is_query=True.\n",
    "    - For documents, is_query can be False or omitted.\n",
    "    - To get Matryoshka embeddings, specify the desired dimension with `dim`.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"inputs\": texts,\n",
    "        \"is_query\": is_query\n",
    "    }\n",
    "    if dim > 0:\n",
    "        payload[\"dim\"] = dim\n",
    "    \n",
    "    response = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    # The model returns a JSON object with the key \"dense_embeddings\"\n",
    "    json_str = response['Body'].read().decode('utf8')\n",
    "    response_json = json.loads(json_str)\n",
    "    return response_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4f56a-092e-4a6a-a920-48550ec9f20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Example Usage (Default Dimension) ---\n",
    "\n",
    "# 1. Define queries and documents\n",
    "queries = [\n",
    "    \"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "print(\"--- Default Dimension Example ---\")\n",
    "print(\"1. Embedding queries...\")\n",
    "query_response = get_embeddings(queries, is_query=True)\n",
    "query_embeddings = query_response['dense_embeddings']\n",
    "\n",
    "print(\"2. Embedding documents...\")\n",
    "doc_response = get_embeddings(documents)\n",
    "doc_embeddings = doc_response['dense_embeddings']\n",
    "\n",
    "# 4. Calculate similarity scores\n",
    "scores = np.dot(np.array(query_embeddings), np.array(doc_embeddings).T)\n",
    "\n",
    "print(\"\\n--- Results (Default Dimension) ---\")\n",
    "print(\"Query embeddings shape:\", np.array(query_embeddings).shape)\n",
    "print(\"Document embeddings shape:\", np.array(doc_embeddings).shape)\n",
    "print(\"Similarity scores (dot product):\")\n",
    "print(scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e80bc1-7db1-4db4-9b66-2c69962fd882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Matryoshka Embedding Example (dim=1024) ---\n",
    "print(\"\\n--- Matryoshka Embedding Example (dim=1024) ---\")\n",
    "dim = 1024\n",
    "\n",
    "print(f\"1. Embedding queries with dimension {dim}...\")\n",
    "query_response_matryoshka = get_embeddings(queries, is_query=True, dim=dim)\n",
    "query_embeddings_matryoshka = query_response_matryoshka['dense_embeddings']\n",
    "\n",
    "print(f\"2. Embedding documents with dimension {dim}...\")\n",
    "doc_response_matryoshka = get_embeddings(documents, dim=dim)\n",
    "doc_embeddings_matryoshka = doc_response_matryoshka['dense_embeddings']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637ec62-e5a9-43e6-88c6-e97876e4bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity scores\n",
    "scores_matryoshka = np.dot(np.array(query_embeddings_matryoshka), np.array(doc_embeddings_matryoshka).T)\n",
    "\n",
    "print(f\"\\n--- Results (dim={dim}) ---\")\n",
    "print(\"Query embeddings shape:\", np.array(query_embeddings_matryoshka).shape)\n",
    "print(\"Document embeddings shape:\", np.array(doc_embeddings_matryoshka).shape)\n",
    "print(\"Similarity scores (dot product):\")\n",
    "print(scores_matryoshka.tolist())\n",
    "\n",
    "print(\"\\nMost similar document for each query (using default dimension embeddings):\")\n",
    "for i, query in enumerate(queries):\n",
    "    most_similar_doc_index = np.argmax(scores[i])\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Most similar document: '{documents[most_similar_doc_index]}'\")\n",
    "    print(f\"Score: {scores[i][most_similar_doc_index]:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
