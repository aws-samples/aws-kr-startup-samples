{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93179240-9c5f-4ba6-a1c7-3a981624f794",
   "metadata": {},
   "source": [
    "# Ingest massive amounts of data to a Vector DB (Amazon OpenSearch Serverless)\n",
    "**_Use of Amazon OpenSearch Serverless as a vector database for storing embeddings_**\n",
    "\n",
    "This notebook works well on `ml.t3.xlarge` instance with `Python3` kernel from **JupyterLab** or `Data Science 3.0` kernel from **SageMaker Studio Class**.\n",
    "\n",
    "Here is a list of packages that are used in this notebook.\n",
    "\n",
    "```\n",
    "!pip list | grep -E -w \"sagemaker|sagemaker_studio_image_build|langchain|opensearch-py|numpy|sh\"\n",
    "------------------------------------------------------------------------------------------------\n",
    "langchain                            0.1.16\n",
    "langchain-aws                        0.1.6\n",
    "langchain-community                  0.0.34\n",
    "langchain-core                       0.1.52\n",
    "langchain-text-splitters             0.0.2\n",
    "numpy                                1.26.4\n",
    "opensearch-py                        2.2.0\n",
    "sagemaker                            2.215.0\n",
    "sagemaker_studio_image_build         0.6.0\n",
    "sh                                   2.0.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aae52c-cd7a-4637-a07d-9c0131dc7d0a",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e64f84-b7ac-427d-b5a8-cf98b430be9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "!pip install -U pip\n",
    "\n",
    "!pip install -U langchain==0.1.16 langchain-community==0.0.34\n",
    "!pip install -U \"boto3>=1.26.159\" langchain-aws==0.1.6\n",
    "!pip install -U opensearch-py==3.3.0\n",
    "!pip install -U SQLAlchemy==2.0.28\n",
    "!pip install -U sagemaker-studio-image-build==0.6.0\n",
    "!pip install -U sh==2.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8ab9f-fc0e-4866-a981-01288bf33883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip list | grep -E -w \"sagemaker|sagemaker_studio_image_build|langchain|opensearch-py|numpy|sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017bc3f-e507-4f0c-b640-ea774c5ea9c8",
   "metadata": {},
   "source": [
    "## Step 2: Download the data from the web and upload to S3\n",
    "\n",
    "In this step we use `wget` to crawl a Python documentation style website data. All files other than `html`, `txt` and `md` are removed. **This data download would take a few minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b8c14-0ffc-4090-adf1-c2a8a1bdebaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WEBSITE = \"https://sagemaker.readthedocs.io/en/stable/\"\n",
    "DOMAIN = \"sagemaker.readthedocs.io\"\n",
    "DATA_DIR = \"docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb232ee-6b62-4718-9104-345fe7978703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python ./scripts/get_data.py --website {WEBSITE} --domain {DOMAIN} --output-dir {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1fbb8-583a-4c41-a831-715e4250ff3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "aws_region = boto3.Session().region_name\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c127969-4abc-4a31-8829-c00bee321a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREATE_OS_INDEX_HINT_FILE = \"_create_index_hint\"\n",
    "app_name = 'llm-app-rag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25217f27-4995-4da5-8fc4-b1b9533185b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dummy file called _create_index to provide a hint for opensearch index creation\n",
    "# this is needed for Sagemaker Processing Job when there are multiple instance nodes\n",
    "# all running the same code for data ingestion but only one node needs to create the index\n",
    "!touch {DATA_DIR}/{CREATE_OS_INDEX_HINT_FILE}\n",
    "\n",
    "# upload this data to S3, to be used when we run the Sagemaker Processing Job\n",
    "!aws s3 cp --recursive {DATA_DIR}/ s3://{bucket}/{app_name}/{DOMAIN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ae926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def get_cfn_outputs(stackname: str, region_name: str) -> List:\n",
    "    cfn = boto3.client('cloudformation', region_name=region_name)\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9810194",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFN_STACK_NAME = \"RAGOpenSearchServerlessStack\"\n",
    "cfn_stack_outputs = get_cfn_outputs(CFN_STACK_NAME, aws_region)\n",
    "\n",
    "opensearch_domain_endpoint = cfn_stack_outputs['OpenSearchDomainEndpoint']\n",
    "opensearch_index = 'llm_rag_embeddings'\n",
    "\n",
    "opensearch_domain_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE_FOR_DOC_SPLIT = 600\n",
    "CHUNK_OVERLAP_FOR_DOC_SPLIT = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e90101",
   "metadata": {},
   "source": [
    "## Step 3: Load data into Amazon OpenSearch Serverless\n",
    "\n",
    "- Option 1) Parallel loading data with SageMaker Processing Job\n",
    "- Option 2) Sequential loading data with Document Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15667be7-43b5-4954-95e0-885c9173f82c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Option 1) Parallel loading data with SageMaker Processing Job\n",
    "\n",
    "We now have a working script that is able to ingest data into an Amazon OpenSearch Serverless index. But for this to work for massive amounts of data we need to scale up the processing by running this code in a distributed fashion. We will do this using Sagemkaer Processing Job. This involves the following steps:\n",
    "\n",
    "1. Create a custom container in which we will install the `langchain` and `opensearch-py` packges and then upload this container image to Amazon Elastic Container Registry (ECR).\n",
    "2. Use the Sagemaker `ScriptProcessor` class to create a Sagemaker Processing job that will run on multiple nodes.\n",
    "    - The data files available in S3 are automatically distributed across in the Sagemaker Processing Job instances by setting `s3_data_distribution_type='ShardedByS3Key'` as part of the `ProcessingInput` provided to the processing job.\n",
    "    - Each node processes a subset of the files and this brings down the overall time required to ingest the data into Opensearch.\n",
    "    - Each node also uses Python `multiprocessing` to internally also parallelize the file processing. Thus, **there are two levels of parallelization happening, one at the cluster level where individual nodes are distributing the work (files) amongst themselves and another at the node level where the files in a node are also split between multiple processes running on the node**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d2938-994f-432d-8c50-92269f22f4b8",
   "metadata": {},
   "source": [
    "### Create custom container\n",
    "\n",
    "We will now create a container locally and push the container image to ECR. **The container creation process takes about 1 minute**.\n",
    "\n",
    "1. The container include all the Python packages we need i.e. `langchain`, `opensearch-py`, `sagemaker` and `beautifulsoup4`.\n",
    "1. The container also includes the `credentials.py` script for retrieving credentials from Secrets Manager and `sm_helper.py` for helping to create SageMaker endpoint classes that langchain uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01848619-8b49-48da-8cbf-c9cbbd8d1e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOCKER_IMAGE = \"load-data-opensearch-custom\"\n",
    "DOCKER_IMAGE_TAG = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd70a7-8f61-477c-a8ef-82c0b5cd7821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd ./container && sm-docker build . --repository {DOCKER_IMAGE}:{DOCKER_IMAGE_TAG}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1089064e-2099-4226-82c1-c7c406203c49",
   "metadata": {},
   "source": [
    "### Create and run the Sagemaker Processing Job\n",
    "\n",
    "Now we will run the Sagemaker Processing Job to ingest the data into OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7373c-e80f-4d1a-a8dc-0dc79fb28e8a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s,%(module)s,%(processName)s,%(levelname)s,%(message)s', level=logging.INFO, stream=sys.stderr)\n",
    "\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "# setup taws_regionmeters for the job\n",
    "base_job_name = f\"{app_name}-job\"\n",
    "tags = [{\"Key\": \"data\", \"Value\": \"embeddings-for-llm-apps\"}]\n",
    "\n",
    "# use the custom container we just created\n",
    "image_uri = f\"{account_id}.dkr.ecr.{aws_region}.amazonaws.com/{DOCKER_IMAGE}:{DOCKER_IMAGE_TAG}\"\n",
    "\n",
    "# instance type and count determined via trial and error: how much overall processing time\n",
    "# and what compute cost works best for your use-case\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "instance_count = 3\n",
    "logger.info(f\"base_job_name={base_job_name}, tags={tags}, image_uri={image_uri}, instance_type={instance_type}, instance_count={instance_count}\")\n",
    "\n",
    "# setup the ScriptProcessor with the above parameters\n",
    "processor = ScriptProcessor(base_job_name=base_job_name,\n",
    "                            image_uri=image_uri,\n",
    "                            role=aws_role,\n",
    "                            instance_type=instance_type,\n",
    "                            instance_count=instance_count,\n",
    "                            command=[\"python3\"],\n",
    "                            tags=tags)\n",
    "\n",
    "# setup input from S3, note the ShardedByS3Key, this ensures that\n",
    "# each instance gets a random and equal subset of the files in S3.\n",
    "inputs = [ProcessingInput(source=f\"s3://{bucket}/{app_name}/{DOMAIN}\",\n",
    "                          destination='/opt/ml/processing/input_data',\n",
    "                          s3_data_distribution_type='ShardedByS3Key',\n",
    "                          s3_data_type='S3Prefix')]\n",
    "\n",
    "\n",
    "logger.info(f\"creating an opensearch index with name={opensearch_index}\")\n",
    "\n",
    "# ready to run the processing job\n",
    "st = time.time()\n",
    "processor.run(code=\"container/load_data_into_opensearch.py\",\n",
    "              inputs=inputs,\n",
    "              outputs=[],\n",
    "              arguments=[\"--opensearch-cluster-domain\", opensearch_domain_endpoint,\n",
    "                         \"--opensearch-index-name\", opensearch_index,\n",
    "                         \"--aws-region\", aws_region,\n",
    "                         \"--chunk-size-for-doc-split\", str(CHUNK_SIZE_FOR_DOC_SPLIT),\n",
    "                         \"--chunk-overlap-for-doc-split\", str(CHUNK_OVERLAP_FOR_DOC_SPLIT),\n",
    "                         \"--input-data-dir\", \"/opt/ml/processing/input_data\",\n",
    "                         \"--create-index-hint-file\", CREATE_OS_INDEX_HINT_FILE,\n",
    "                         \"--process-count\", \"2\"])\n",
    "\n",
    "time_taken = time.time() - st\n",
    "logger.info(f\"processing job completed, total time taken={time_taken}s\")\n",
    "\n",
    "preprocessing_job_description = processor.jobs[-1].describe()\n",
    "logger.info(preprocessing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789061da",
   "metadata": {},
   "source": [
    "### Option 2) Sequential loading data with Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "!pip install -Uq beautifulsoup4==4.12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ea828",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from langchain_community.document_loaders import ReadTheDocsLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "\n",
    "\n",
    "loader = ReadTheDocsLoader(DATA_DIR)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE_FOR_DOC_SPLIT,\n",
    "    chunk_overlap=CHUNK_OVERLAP_FOR_DOC_SPLIT,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# add a custom metadata field, such as timestamp\n",
    "for doc in docs:\n",
    "    doc.metadata['timestamp'] = time.time()\n",
    "    doc.metadata['embeddings_model'] = 'amazon.titan-embed-text-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.create_documents(\n",
    "  [doc.page_content for doc in docs],\n",
    "  metadatas=[doc.metadata for doc in docs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47746ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "MAX_OS_DOCS_PER_PUT = 500\n",
    "\n",
    "db_shards = (len(chunks) // MAX_OS_DOCS_PER_PUT) + 1\n",
    "shards = np.array_split(chunks, db_shards)\n",
    "\n",
    "print(f'Loading chunks into vector store ... using {len(shards)} shards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id='amazon.titan-embed-text-v1',\n",
    "    region_name=aws_region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b506af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from container.credentials import get_auth\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "\n",
    "CONNECTION_TIMEOUT = 1000\n",
    "\n",
    "http_auth = get_auth(aws_region)\n",
    "\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    index_name=opensearch_index,\n",
    "    documents=shards[0],\n",
    "    embedding=embeddings,\n",
    "    opensearch_url=opensearch_domain_endpoint,\n",
    "    http_auth=http_auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=CONNECTION_TIMEOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i, shard in enumerate(shards[1:]):\n",
    "    docsearch.add_documents(documents=shard)\n",
    "    print(f\"[{i+1}] shard is added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e444161-262e-44e5-ad31-e490a763be4e",
   "metadata": {},
   "source": [
    "## Step 4: Do a similarity search for user input to documents (embeddings) in OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602d548-7bc5-4d4d-94a2-2269a052c299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id='amazon.titan-embed-text-v1',\n",
    "    region_name=aws_region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a7292-8bdb-4d11-a23d-130a4a039cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from container.credentials import get_auth\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "\n",
    "http_auth = get_auth(aws_region)\n",
    "\n",
    "docsearch = OpenSearchVectorSearch(index_name=opensearch_index,\n",
    "                                   embedding_function=embeddings,\n",
    "                                   opensearch_url=opensearch_domain_endpoint,\n",
    "                                   http_auth=http_auth,\n",
    "                                   use_ssl=True,\n",
    "                                   verify_certs=True,\n",
    "                                   connection_class=RequestsHttpConnection,\n",
    "                                   timeout=1000)\n",
    "\n",
    "q = \"Which XGBoost versions does SageMaker support?\"\n",
    "docs = docsearch.similarity_search(q, k=3) #, search_type=\"script_scoring\", space_type=\"cosinesimil\"\n",
    "for doc in docs:\n",
    "    logger.info(\"----------\")\n",
    "    logger.info(f\"content=\\\"{doc.page_content}\\\",\\nmetadata=\\\"{doc.metadata}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29eae5-c463-4153-9167-e4628c74d13c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "To avoid incurring future charges, delete the resources. You can do this by deleting the CloudFormation template used to create the IAM role and SageMaker notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce3fe8-bb71-4e22-a551-2475eb2d16b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "In this notebook we were able to see how to use LLMs deployed on a SageMaker Endpoint to generate embeddings and then ingest those embeddings into OpenSearch and finally do a similarity search for user input to the documents (embeddings) stored in OpenSearch. We used langchain as an abstraction layer to talk to both the SageMaker Endpoint as well as OpenSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53386268-0cf9-4a37-b3d0-711fba1e5585",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332274c-586d-4cf9-838f-ea7e0cbe6c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id='amazon.titan-embed-text-v1',\n",
    "    region_name=aws_region\n",
    ")\n",
    "\n",
    "text = \"This is a sample query.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "\n",
    "print(np.array(query_result))\n",
    "print(f\"length: {len(query_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd881bab",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "  * [Build a powerful question answering bot with Amazon SageMaker, Amazon OpenSearch Service, Streamlit, and LangChain](https://aws.amazon.com/blogs/machine-learning/build-a-powerful-question-answering-bot-with-amazon-sagemaker-amazon-opensearch-service-streamlit-and-langchain/)\n",
    "  * [Using the Amazon SageMaker Studio Image Build CLI to build container images from your Studio notebooks](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/)\n",
    "  * [LangChain](https://python.langchain.com/docs/get_started/introduction.html) - A framework for developing applications powered by language models."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:123456789012:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
