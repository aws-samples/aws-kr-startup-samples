{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Knowledge Bases가 제공하는 고급 청킹 전략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북에서는 Amazon Bedrock Knowledge Bases가 지원하는 다음 청킹 옵션을 시연하기 위해 3개의 Knowledge Base를 생성합니다: \n",
    "1. 고정 청킹\n",
    "2. 세만틱 청킹\n",
    "3. 계층형 청킹\n",
    "4. Lambda 함수를 활용한 사용자 정의 청킹\n",
    "\n",
    "청킹은 임베딩 전에 텍스트를 작은 조각으로 나누는 과정입니다. 데이터 소스를 생성한 이후에는 청킹 전략을 변경할 수 없습니다. 현재 Amazon Bedrock Knowledge Bases는 기본적으로 청킹을 사용하지 않음, 고정 크기 청킹, 기본 청킹 등 일부 내장 옵션만 지원합니다.\n",
    "\n",
    "세만틱 청킹과 계층형 청킹 기능(기존 옵션과 함께)을 사용하면 고객은 Lambda 함수를 통해 데이터가 처리되고 청킹되는 방식을 더 세밀하게 제어할 수 있습니다.\n",
    "\n",
    "데모에는 `Octank Financial`이라는 가상의 회사에 대한 10-K 보고서를 사용합니다. Knowledge Base를 생성한 뒤 동일한 데이터 세트에서 결과를 평가하여 검색 품질을 향상시키고, 그에 따라 Foundation Model이 생성하는 응답의 정확도를 높이는 데 집중합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 불러오기\n",
    "먼저 필요한 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip --quiet\n",
    "%pip install -r ../requirements.txt --no-deps --quiet\n",
    "%pip install -r ../requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ragas==0.1.9 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "botocore.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "# Set the path to import module\n",
    "from pathlib import Path\n",
    "current_path = Path().resolve()\n",
    "current_path = current_path.parent\n",
    "if str(current_path) not in sys.path:\n",
    "    sys.path.append(str(current_path))\n",
    "# Print sys.path to verify\n",
    "# print(sys.path)\n",
    "\n",
    "from utils.knowledge_base import BedrockKnowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clients\n",
    "s3_client = boto3.client('s3')\n",
    "sts_client = boto3.client('sts')\n",
    "session = boto3.session.Session()\n",
    "region =  session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime') \n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "region, account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Get the current timestamp\n",
    "current_time = time.time()\n",
    "\n",
    "# Format the timestamp as a string\n",
    "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
    "# Create the suffix using the timestamp\n",
    "suffix = f\"{timestamp_str}\"\n",
    "knowledge_base_name_standard = 'standard-kb'\n",
    "knowledge_base_name_hierarchical = 'hierarchical-kb'\n",
    "knowledge_base_name_semantic = 'semantic-kb'\n",
    "knowledge_base_name_custom = 'custom-chunking-kb'\n",
    "knowledge_base_description = \"Knowledge Base containing complex PDF.\"\n",
    "bucket_name = f'{knowledge_base_name_standard}-{suffix}'\n",
    "intermediate_bucket_name = f'{knowledge_base_name_standard}-intermediate-{suffix}'\n",
    "lambda_function_name = f'{knowledge_base_name_custom}-lambda-{suffix}'\n",
    "foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "data_source=[{\"type\": \"S3\", \"bucket_name\": bucket_name}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 고정 청킹 전략으로 Knowledge Base 생성\n",
    "먼저 레스토랑 메뉴를 저장할 [Amazon Bedrock Knowledge Bases](https://aws.amazon.com/bedrock/knowledge-bases/)를 생성해 보겠습니다. Knowledge Bases는 [Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/features/serverless/), [Amazon Aurora](https://aws.amazon.com/rds/aurora/), [Pinecone](http://app.pinecone.io/bedrock-integration), [Redis Enterprise]() 및 [MongoDB Atlas]() 등 다양한 벡터 데이터베이스와 통합할 수 있습니다. 이 예제에서는 Knowledge Base를 Amazon OpenSearch Serverless와 연동합니다. 이를 위해 Knowledge Base와 필요한 사전 준비 작업을 모두 생성해 주는 헬퍼 클래스 `BedrockKnowledgeBase`를 사용합니다:\n",
    "1. IAM 역할 및 정책\n",
    "2. S3 버킷\n",
    "3. Amazon OpenSearch Serverless 암호화, 네트워크 및 데이터 접근 정책\n",
    "4. Amazon OpenSearch Serverless 컬렉션\n",
    "5. Amazon OpenSearch Serverless 벡터 인덱스\n",
    "6. Knowledge Base\n",
    "7. Knowledge Base 데이터 소스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 고정 청킹 전략으로 Knowledge Base를 생성한 다음 계층형 청킹 전략을 적용합니다. \n",
    "\n",
    "매개변수 값: \n",
    "```\n",
    "\"chunkingStrategy\": \"FIXED_SIZE | NONE | HIERARCHICAL | SEMANTIC\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_standard = BedrockKnowledgeBase(\n",
    "    kb_name=f'{knowledge_base_name_standard}-{suffix}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_sources=data_source,\n",
    "    chunking_strategy = \"FIXED_SIZE\", \n",
    "    suffix = f'{suffix}-f'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 데이터 세트를 Amazon S3에 업로드\n",
    "Knowledge Base를 만들었으니 `Octank financial 10K` 보고서 데이터 세트로 내용을 채워 보겠습니다. Knowledge Base 데이터 소스는 연결된 S3 버킷에 데이터가 있어야 하며, 데이터 변경 사항은 `StartIngestionJob` API 호출을 통해 Knowledge Base와 동기화할 수 있습니다. 이 예제에서는 헬퍼 클래스를 통해 API의 [boto3 추상화](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent/client/start_ingestion_job.html)를 사용합니다. \n",
    "\n",
    "먼저 `dataset` 폴더에 있는 데이터를 S3에 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def upload_directory(path, bucket_name):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_to_upload = os.path.join(root, file)\n",
    "            if file not in [\"LICENSE\", \"NOTICE\", \"README.md\"]:\n",
    "                print(f\"uploading file {file_to_upload} to {bucket_name}\")\n",
    "                s3_client.upload_file(file_to_upload, bucket_name, file)\n",
    "            else:\n",
    "                print(f\"Skipping file {file_to_upload}\")\n",
    "\n",
    "upload_directory(\"../synthetic_dataset\", bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 ingestion 작업을 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the kb is available\n",
    "time.sleep(30)\n",
    "# sync knowledge base\n",
    "knowledge_base_standard.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 나중에 솔루션을 테스트할 수 있도록 Knowledge Base ID를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id_standard = knowledge_base_standard.get_knowledge_base_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Knowledge Base 테스트\n",
    "Knowledge Base가 준비되었으므로 [**retrieve**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html)와 [**retrieve_and_generate**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) 함수를 사용해 확인할 수 있습니다. \n",
    "\n",
    "#### Retrieve and Generate API로 Knowledge Base 테스트\n",
    "\n",
    "먼저 retrieve and generate API로 Knowledge Base를 시험해 보겠습니다. 이 API는 Bedrock이 Knowledge Base에서 필요한 참조를 검색하고 Bedrock의 Foundation Model로 최종 답변을 생성합니다.\n",
    "\n",
    "query = `Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.`\n",
    "\n",
    "해당 질의의 정답(ground truth QA 쌍 기준)은 다음과 같습니다: \n",
    "\n",
    "```\n",
    "The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\n",
    "- Cash generated from operating activities amounted to $710 million, which can be attributed to a $700 million profit and non-cash charges such as depreciation and amortization.\n",
    "- Cash outflow from investing activities totaled $240 million, with major expenditures being the acquisition of property, plant, and equipment (\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(20)\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_standard,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 Retrieve and Generate API를 사용하면 최종 응답을 바로 받을 수 있습니다. 이제 `RetrieveAndGenerate` API에서 제공하는 인용 정보를 살펴보고, 응답을 생성할 때 모델이 반환하는 검색된 청크와 인용을 확인해 보겠습니다. 질의와 함께 적절한 컨텍스트를 Foundation Model에 제공하면 고품질 응답이 생성될 가능성이 크게 높아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citations_rag_print(response_ret):\n",
    "#structure 'retrievalResults': list of contents. Each list has content, location, score, metadata\n",
    "    for num,chunk in enumerate(response_ret,1):\n",
    "        print(f'Chunk {num}: ',chunk['content']['text'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Location: ',chunk['location'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Metadata: ',chunk['metadata'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_standard = response['citations'][0]['retrievedReferences']\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_standard))\n",
    "citations_rag_print(response_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Base에서 제공하는 소스 정보를 Retrieve API로 확인해 보겠습니다.\n",
    "\n",
    "#### Retrieve API로 Knowledge Base 테스트\n",
    "추가적인 제어가 필요하다면 Retrieve API를 사용해 질의와 가장 잘 일치하는 청크를 직접 가져올 수 있습니다. 이 설정에서는 원하는 결과 수를 구성하고 자체 애플리케이션 로직으로 최종 답변을 제어할 수 있습니다. API는 일치하는 콘텐츠, 해당 S3 위치, 유사도 점수, 청크 메타데이터를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_print(response_ret):\n",
    "#structure 'retrievalResults': list of contents. Each list has content, location, score, metadata\n",
    "    for num,chunk in enumerate(response_ret['retrievalResults'],1):\n",
    "        print(f'Chunk {num}: ',chunk['content']['text'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Location: ',chunk['location'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Score: ',chunk['score'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Metadata: ',chunk['metadata'],end='\\n'*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_standard_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id_standard, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":5,\n",
    "        } \n",
    "    },\n",
    "    retrievalQuery={\n",
    "        'text': query\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"# of retrieved results: \", len(response_standard_ret['retrievalResults']))\n",
    "response_print(response_standard_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "고정 청킹을 사용할 경우 기본값인 `semantic similarity`로 5개의 검색 결과를 요청하면, API가 5개의 결과를 반환하는 것을 확인할 수 있습니다. 이제 `hierarchical chunking` 전략을 적용한 뒤 `RetrieveAndGenerate` API와 `Retrieve` API로 검색 결과를 비교해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 계층형 청킹 전략으로 Knowledge Base 생성\n",
    "\n",
    "**개념**\n",
    "\n",
    "계층형 청킹은 데이터를 계층 구조로 구성하여 데이터에 존재하는 관계를 기반으로 더 세밀하고 효율적인 검색이 가능하도록 합니다. 데이터가 계층 구조로 구성되면 RAG 워크플로가 복잡하고 중첩된 데이터 세트에서도 정보를 효율적으로 탐색하고 가져올 수 있습니다.\n",
    "문서가 파싱된 후 첫 단계로 부모 청크와 자식 청크 크기에 따라 문서를 분할합니다. 이후 청크는 계층 구조로 정리되며, 부모 청크(상위 레벨)는 더 큰 단위(예: 문서나 섹션)를, 자식 청크(하위 레벨)는 더 작은 단위(예: 문단이나 문장)를 나타냅니다. 부모와 자식 청크 간의 관계는 유지되며, 이 구조 덕분에 코퍼스를 효율적으로 탐색하고 검색할 수 있습니다.\n",
    "\n",
    "**이점**\n",
    "\n",
    "- 효율적인 검색: 계층 구조는 먼저 자식 청크에 대해 세만틱 검색을 수행한 뒤 검색 시 부모 청크를 반환하므로 관련 정보를 빠르고 정확하게 찾아낼 수 있습니다. 자식 청크를 부모 청크로 교체해 제공함으로써 Foundation Model에 더 크고 포괄적인 컨텍스트를 전달합니다.\n",
    "- 컨텍스트 보존: 코퍼스를 계층적으로 구성하면 청크 간의 컨텍스트 관계가 유지되어, 일관성 있고 컨텍스트에 맞는 텍스트 생성에 도움이 됩니다.\n",
    "\n",
    "><br>          \n",
    ">참고:\n",
    ">계층형 청킹에서는 부모 청크가 반환되고 검색은 자식 청크에서 수행되므로, 요청한 결과 수보다 적은 검색 결과가 반환될 수 있습니다. 하나의 부모 청크에는 여러 자식 청크가 포함될 수 있기 때문입니다.\n",
    "><br></br>       \n",
    "\n",
    "계층형 청킹은 기술 매뉴얼, 법률 문서, 복잡한 형식과 중첩된 테이블을 포함한 학술 문서처럼 계층적 구조가 있는 복잡한 문서에 특히 적합합니다.\n",
    "\n",
    "**매개변수 값:** \n",
    "```\n",
    "\"chunkingStrategy\": \"FIXED_SIZE | NONE | HIERARCHICAL | SEMANTIC\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_hierarchical = BedrockKnowledgeBase(\n",
    "    kb_name=f'{knowledge_base_name_hierarchical}-{suffix}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_sources=data_source,\n",
    "    chunking_strategy = \"HIERARCHICAL\", \n",
    "    suffix = f'{suffix}-h'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 ingestion 작업을 시작합니다. 이미 고정 청킹에서 사용한 것과 동일한 문서를 사용하므로, S3 버킷에 문서를 업로드하는 단계는 건너뜁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the kb is available\n",
    "time.sleep(30)\n",
    "# sync knowledge base\n",
    "knowledge_base_hierarchical.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가 테스트를 위해 Knowledge Base ID를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id_hierarchical = knowledge_base_hierarchical.get_knowledge_base_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Knowledge Base 테스트\n",
    "Knowledge Base가 준비되었으므로 [**retrieve**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html)와 [**retrieve_and_generate**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) 함수를 사용해 확인할 수 있습니다. \n",
    "\n",
    "#### Retrieve and Generate API로 Knowledge Base 테스트\n",
    "\n",
    "먼저 retrieve and generate API로 Knowledge Base를 시험해 보겠습니다. 이 API는 Bedrock이 Knowledge Base에서 필요한 참조를 검색하고 Bedrock의 Foundation Model로 최종 답변을 생성합니다.\n",
    "\n",
    "query = `Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.`\n",
    "\n",
    "해당 질의의 정답(ground truth QA 쌍 기준)은 다음과 같습니다: \n",
    "\n",
    "```\n",
    "The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(20)\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_hierarchical,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 Retrieve and Generate API를 사용하면 최종 응답을 바로 받을 수 있습니다. 이제 `RetrieveAndGenerate` API에서 제공하는 인용 정보를 살펴보고, 응답을 생성할 때 모델이 반환하는 검색된 청크와 인용을 확인해 보겠습니다. 질의와 함께 적절한 컨텍스트를 Foundation Model에 제공하면 고품질 응답이 생성될 가능성이 크게 높아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_hierarchical = response['citations'][0]['retrievedReferences']\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_hierarchical))\n",
    "citations_rag_print(response_hierarchical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Base에서 제공하는 소스 정보를 Retrieve API로 확인해 보겠습니다.\n",
    "\n",
    "#### Retrieve API로 Knowledge Base 테스트\n",
    "추가적인 제어가 필요하다면 Retrieve API를 사용해 질의와 가장 잘 일치하는 청크를 직접 가져올 수 있습니다. 이 설정에서는 원하는 결과 수를 구성하고 자체 애플리케이션 로직으로 최종 답변을 제어할 수 있습니다. API는 일치하는 콘텐츠, 해당 S3 위치, 유사도 점수, 청크 메타데이터를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_hierarchical_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id_hierarchical, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":5,\n",
    "        } \n",
    "    },\n",
    "    retrievalQuery={\n",
    "        'text': query\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"# of retrieved results: \", len(response_hierarchical_ret['retrievalResults']))\n",
    "response_print(response_hierarchical_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><br>\n",
    "> 참고:\n",
    "> 위 응답에서 볼 수 있듯이 `retrieve` API는 요청에서 5개의 결과를 전달했지만 3개의 검색 결과(청크)만 반환했습니다. `hierarchical` 청킹에서는 검색이 자식 청크에서 수행되고 API는 부모 청크를 반환하므로, 하나의 부모 청크에 여러 자식 청크가 포함될 수 있습니다. 따라서 검색은 5개의 자식 청크에 대해 수행되었으나 응답에는 3개의 부모 청크만 포함되었습니다.\n",
    "><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 세만틱 청킹 전략으로 Knowledge Base 생성\n",
    "\n",
    "**개념**\n",
    "\n",
    "세만틱 청킹은 텍스트 내부의 관계를 분석해 임베딩 모델이 계산한 의미적 유사성에 기반하여 의미 있고 완전한 청크로 분할합니다. 이 접근 방식은 검색 과정에서 정보의 온전성을 유지해 정확하고 문맥에 맞는 결과를 제공합니다.\n",
    "Amazon Bedrock Knowledge Bases는 먼저 지정된 토큰 크기에 따라 문서를 청크로 분할합니다. 각 청크에 대해 임베딩을 생성하고, 임베딩 공간에서 유사성 임계값과 버퍼 크기를 기준으로 유사한 청크를 결합해 새로운 청크를 형성합니다. 그 결과 청크 크기는 청크마다 달라질 수 있습니다.\n",
    "\n",
    "**이점**\n",
    "\n",
    "- 텍스트의 의미와 컨텍스트에 집중함으로써 세만틱 청킹은 검색 품질을 크게 향상시킵니다. 텍스트의 의미적 일관성을 유지하는 것이 중요한 상황에서 사용하기 좋습니다.\n",
    "\n",
    "- 이 방법은 고정 크기 청킹보다 계산 비용이 더 들지만, 컨텍스트 경계가 명확하지 않은 문서(예: 법률 문서, 기술 매뉴얼)를 청킹하는 데 유용합니다.[[1]](#https://www.mongodb.com/developer/products/atlas/choosing-chunking-strategy-rag/)\n",
    "\n",
    "**매개변수 값:**\n",
    "```\n",
    "\"chunkingStrategy\": \"FIXED_SIZE | NONE | HIERARCHICAL | SEMANTIC\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_semantic = BedrockKnowledgeBase(\n",
    "    kb_name=f'{knowledge_base_name_semantic}-{suffix}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_sources=data_source, \n",
    "    chunking_strategy = \"SEMANTIC\", \n",
    "    suffix = f'{suffix}-s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 ingestion 작업을 시작합니다. 이미 고정 청킹에서 사용한 것과 동일한 문서를 사용하므로, S3 버킷에 문서를 업로드하는 단계는 건너뜁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the kb is available\n",
    "time.sleep(30)\n",
    "# sync knowledge base\n",
    "knowledge_base_semantic.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id_semantic = knowledge_base_semantic.get_knowledge_base_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Knowledge Base 테스트\n",
    "Knowledge Base가 준비되었으므로 [**retrieve**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html)와 [**retrieve_and_generate**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) 함수를 사용해 확인할 수 있습니다. \n",
    "\n",
    "#### Retrieve and Generate API로 Knowledge Base 테스트\n",
    "\n",
    "먼저 retrieve and generate API로 Knowledge Base를 시험해 보겠습니다. 이 API는 Bedrock이 Knowledge Base에서 필요한 참조를 검색하고 Bedrock의 Foundation Model로 최종 답변을 생성합니다.\n",
    "\n",
    "query = `Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.`\n",
    "\n",
    "해당 질의의 정답(ground truth QA 쌍 기준)은 다음과 같습니다: \n",
    "\n",
    "```\n",
    "The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\n",
    "- Cash generated from operating activities amounted to $710 million, which can be attributed to a $700 million profit and non-cash charges such as depreciation and amortization.\n",
    "- Cash outflow from investing activities totaled $240 million, with major expenditures being the acquisition of property, plant, and equipment ($200 million) and marketable securities ($60 million), partially offset by the sale of property, plant, and equipment ($40 million) and maturing marketable securities ($20 million).\n",
    "- Financing activities resulted in a cash inflow of $350 million, stemming from the issuance of common stock ($200 million) and long-term debt ($300 million), while common stock repurchases ($50 million) and long-term debt payments ($100 million) reduced the cash inflow. \n",
    "Overall, Octank Financial experienced a net cash enhancement of $120 million in 2019, bringing their total cash and cash equivalents to $210 million.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(20)\n",
    "\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_semantic,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 Retrieve and Generate API를 사용하면 최종 응답을 바로 받을 수 있습니다. 이제 `RetrieveAndGenerate` API에서 제공하는 인용 정보를 살펴보고, 응답을 생성할 때 모델이 반환하는 검색된 청크와 인용을 확인해 보겠습니다. 질의와 함께 적절한 컨텍스트를 Foundation Model에 제공하면 고품질 응답이 생성될 가능성이 크게 높아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_semantic = response['citations'][0]['retrievedReferences']\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_semantic))\n",
    "citations_rag_print(response_semantic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Base에서 제공하는 소스 정보를 Retrieve API로 확인해 보겠습니다.\n",
    "\n",
    "#### Retrieve API로 Knowledge Base 테스트\n",
    "추가적인 제어가 필요하다면 Retrieve API를 사용해 질의와 가장 잘 일치하는 청크를 직접 가져올 수 있습니다. 이 설정에서는 원하는 결과 수를 구성하고 자체 애플리케이션 로직으로 최종 답변을 제어할 수 있습니다. API는 일치하는 콘텐츠, 해당 S3 위치, 유사도 점수, 청크 메타데이터를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_semantic_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id_semantic, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":5,\n",
    "        } \n",
    "    },\n",
    "    retrievalQuery={\n",
    "        'text': query\n",
    "    }\n",
    ")\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_semantic_ret['retrievalResults']))\n",
    "response_print(response_semantic_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lambda 함수를 사용하는 사용자 정의 청킹 옵션\n",
    "Amazon Bedrock용 Knowledge Base(KB)를 생성할 때 Lambda 함수를 연결해 사용자 지정 청킹 로직을 정의할 수 있습니다. Ingestion 동안 Lambda 함수가 제공되면 Knowledge Bases는 해당 함수를 실행하고 입력 및 출력 값을 제공한 중간 S3 버킷에 저장합니다.\n",
    "\n",
    "> <br>\n",
    "> 참고: Lambda 함수는 사용자 정의 청킹 로직을 추가하는 용도뿐만 아니라 청크 수준 메타데이터를 추가하는 등 청크 후처리를 수행하는 데에도 사용할 수 있습니다. 이 예제에서는 Lambda 함수를 사용자 정의 청킹 로직에 초점을 맞춰 사용합니다.\n",
    "> <br></br>\n",
    "\n",
    "### 5.1 Lambda 함수 생성\n",
    "\n",
    "이제 사용자 정의 청킹 로직이 포함된 Lambda 함수를 생성하겠습니다. 이를 위해 다음 작업을 수행합니다:\n",
    "\n",
    "1. 사용자 정의 청킹 로직을 포함하는 `lambda_function.py` 파일을 생성합니다.\n",
    "2. Lambda 함수용 IAM 역할을 생성합니다.\n",
    "3. 필요한 권한과 함께 Lambda 함수를 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수 코드 생성\n",
    "Lambda 함수가 중간 버킷에서 파일을 읽고, 사용자 정의 청킹 로직으로 내용을 처리한 뒤 결과를 다시 S3 버킷에 쓰도록 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_function.py\n",
    "import json\n",
    "from abc import abstractmethod, ABC\n",
    "from typing import List\n",
    "from urllib.parse import urlparse\n",
    "import boto3\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "class Chunker(ABC):\n",
    "    @abstractmethod\n",
    "    def chunk(self, text: str) -> List[str]:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class SimpleChunker(Chunker):\n",
    "    def chunk(self, text: str) -> List[str]:\n",
    "        words = text.split()\n",
    "        return [' '.join(words[i:i+100]) for i in range(0, len(words), 100)]\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    logger.debug('input={}'.format(json.dumps(event)))\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Extract relevant information from the input event\n",
    "    input_files = event.get('inputFiles')\n",
    "    input_bucket =  event.get('bucketName')\n",
    "\n",
    "    \n",
    "    if not all([input_files, input_bucket]):\n",
    "        raise ValueError(\"Missing required input parameters\")\n",
    "    \n",
    "    output_files = []\n",
    "    chunker = SimpleChunker()\n",
    "\n",
    "    for input_file in input_files:\n",
    "        content_batches = input_file.get('contentBatches', [])\n",
    "        file_metadata = input_file.get('fileMetadata', {})\n",
    "        original_file_location = input_file.get('originalFileLocation', {})\n",
    "\n",
    "        processed_batches = []\n",
    "        \n",
    "        for batch in content_batches:\n",
    "            input_key = batch.get('key')\n",
    "\n",
    "            if not input_key:\n",
    "                raise ValueError(\"Missing uri in content batch\")\n",
    "            \n",
    "            # Read file from S3\n",
    "            file_content = read_s3_file(s3, input_bucket, input_key)\n",
    "            \n",
    "            # Process content (chunking)\n",
    "            chunked_content = process_content(file_content, chunker)\n",
    "            \n",
    "            output_key = f\"Output/{input_key}\"\n",
    "            \n",
    "            # Write processed content back to S3\n",
    "            write_to_s3(s3, input_bucket, output_key, chunked_content)\n",
    "            \n",
    "            # Add processed batch information\n",
    "            processed_batches.append({\n",
    "                'key': output_key\n",
    "            })\n",
    "        \n",
    "        # Prepare output file information\n",
    "        output_file = {\n",
    "            'originalFileLocation': original_file_location,\n",
    "            'fileMetadata': file_metadata,\n",
    "            'contentBatches': processed_batches\n",
    "        }\n",
    "        output_files.append(output_file)\n",
    "    \n",
    "    result = {'outputFiles': output_files}\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def read_s3_file(s3_client, bucket, key):\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    return json.loads(response['Body'].read().decode('utf-8'))\n",
    "\n",
    "def write_to_s3(s3_client, bucket, key, content):\n",
    "    s3_client.put_object(Bucket=bucket, Key=key, Body=json.dumps(content))    \n",
    "\n",
    "def process_content(file_content: dict, chunker: Chunker) -> dict:\n",
    "    chunked_content = {\n",
    "        'fileContents': []\n",
    "    }\n",
    "    \n",
    "    for content in file_content.get('fileContents', []):\n",
    "        content_body = content.get('contentBody', '')\n",
    "        content_type = content.get('contentType', '')\n",
    "        content_metadata = content.get('contentMetadata', {})\n",
    "        \n",
    "        words = content['contentBody']\n",
    "        chunks = chunker.chunk(words)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunked_content['fileContents'].append({\n",
    "                'contentType': content_type,\n",
    "                'contentMetadata': content_metadata,\n",
    "                'contentBody': chunk\n",
    "            })\n",
    "    \n",
    "    return chunked_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Bases에서 제공하는 표준 청킹 전략 값은 다음과 같습니다: \n",
    "\n",
    "**매개변수 값:**\n",
    "```\n",
    "\"chunkingStrategy\": \"FIXED_SIZE | NONE | HIERARCHICAL | SEMANTIC\"\n",
    "```\n",
    "\n",
    "사용자 정의 로직을 구현하기 위해 `knowledge_base.py` 클래스에 `CUSTOM` 값을 전달할 수 있는 옵션을 추가했습니다. \n",
    "이 클래스에서 청킹 전략을 `CUSTOM`으로 전달하면 다음 작업이 수행됩니다: \n",
    "\n",
    "1. `chunkingStrategy`를 `NONE`으로 설정합니다. \n",
    "2. `vectorIngestionConfiguration`에 `customTransformationConfiguration`을 다음과 같이 추가합니다: \n",
    "\n",
    "```\n",
    "{\n",
    "...\n",
    "   \"vectorIngestionConfiguration\": {\n",
    "    \"customTransformationConfiguration\": { \n",
    "         \"intermediateStorage\": { \n",
    "            \"s3Location\": { \n",
    "               \"uri\": \"string\"\n",
    "            }\n",
    "         },\n",
    "         \"transformations\": [\n",
    "            {\n",
    "               \"transformationFunction\": {\n",
    "                  \"lambdaConfiguration\": {\n",
    "                     \"lambdaArn\": \"string\"\n",
    "                  }\n",
    "               },\n",
    "               \"stepToApply\": \"string\" // enum of POST_CHUNKING\n",
    "            }\n",
    "         ]\n",
    "      },\n",
    "      \"chunkingConfiguration\": {\n",
    "         \"chunkingStrategy\": \"NONE\"\n",
    "         ...\n",
    "   }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_custom = BedrockKnowledgeBase(\n",
    "    kb_name=f'{knowledge_base_name_custom}-{suffix}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_sources=data_source,\n",
    "    lambda_function_name=lambda_function_name,\n",
    "    intermediate_bucket_name=intermediate_bucket_name, \n",
    "    chunking_strategy = \"CUSTOM\", \n",
    "    suffix = f'{suffix}-c'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 ingestion 작업을 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the kb is available\n",
    "time.sleep(30)\n",
    "# sync knowledge base\n",
    "knowledge_base_custom.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id_custom = knowledge_base_custom.get_knowledge_base_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Knowledge Base 테스트\n",
    "Knowledge Base가 준비되었으므로 [**retrieve**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html)와 [**retrieve_and_generate**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) 함수를 사용해 확인할 수 있습니다. \n",
    "\n",
    "#### Retrieve and Generate API로 Knowledge Base 테스트\n",
    "\n",
    "먼저 retrieve and generate API로 Knowledge Base를 시험해 보겠습니다. 이 API는 Bedrock이 Knowledge Base에서 필요한 참조를 검색하고 Bedrock의 Foundation Model로 최종 답변을 생성합니다.\n",
    "\n",
    "query = `Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.`\n",
    "\n",
    "해당 질의의 정답(ground truth QA 쌍 기준)은 다음과 같습니다: \n",
    "\n",
    "```\n",
    "The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_custom,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 Retrieve and Generate API를 사용하면 최종 응답을 바로 받을 수 있습니다. 이제 `RetrieveAndGenerate` API에서 제공하는 인용 정보를 살펴보고, 응답을 생성할 때 모델이 반환하는 검색된 청크와 인용을 확인해 보겠습니다. 질의와 함께 적절한 컨텍스트를 Foundation Model에 제공하면 고품질 응답이 생성될 가능성이 크게 높아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_custom = response['citations'][0]['retrievedReferences']\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_custom))\n",
    "citations_rag_print(response_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Base에서 제공하는 소스 정보를 Retrieve API로 확인해 보겠습니다.\n",
    "\n",
    "#### Retrieve API로 Knowledge Base 테스트\n",
    "추가적인 제어가 필요하다면 Retrieve API를 사용해 질의와 가장 잘 일치하는 청크를 직접 가져올 수 있습니다. 이 설정에서는 원하는 결과 수를 구성하고 자체 애플리케이션 로직으로 최종 답변을 제어할 수 있습니다. API는 일치하는 콘텐츠, 해당 S3 위치, 유사도 점수, 청크 메타데이터를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_custom_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id_custom, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":5,\n",
    "        } \n",
    "    },\n",
    "    retrievalQuery={\n",
    "        'text': query\n",
    "    }\n",
    ")\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_custom_ret['retrievalResults']))\n",
    "response_print(response_custom_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 경우에서 단일 질의를 평가했을 때 올바른 응답을 얻었습니다. 그러나 RAG 애플리케이션을 구축할 때는 정확도 향상을 확인하기 위해 다수의 질문과 답변으로 평가해야 합니다. 다음 단계에서는 오픈소스 프레임워크인 RAG Assessment(RAGAS)를 사용해 `your dataset`에 대한 응답을 평가하고, 컨텍스트 품질 또는 검색 결과 품질과 관련된 지표를 살펴봅니다.\n",
    "우리는 다음 두 가지 지표에 집중합니다: \n",
    "\n",
    "1. Context recall\n",
    "2. Context relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG Assessment(RAGAS) 프레임워크로 데이터 세트의 검색 결과 평가\n",
    "각 청킹 전략에 대해 RAGAS 프레임워크를 사용해 결과를 평가할 수 있습니다. 이 접근 방식은 데이터 세트에 어떤 청킹 전략을 적용해야 하는지 객관적인 근거를 제공해 줍니다. \n",
    "\n",
    "이상적으로는 다른 매개변수도 함께 최적화해야 합니다. 예를 들어 계층형 청킹의 경우 부모 청크와 자식 청크 크기를 다양하게 시도해 보세요. \n",
    "\n",
    "아래 접근 방식은 Amazon Bedrock Knowledge Bases가 권장하는 기본 매개변수에 기반해 어떤 전략을 사용할지에 대한 휴리스틱을 제공합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Semantic: \", kb_id_semantic)\n",
    "print(\"Standard: \", kb_id_standard)\n",
    "print(\"Hierarchical: \", kb_id_hierarchical)\n",
    "print(\"Custom chunking: \", kb_id_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가\n",
    "이 섹션에서는 RAGAS를 사용해 다음 지표로 검색 결과를 평가합니다:\n",
    "1. **Context Recall:** 검색된 컨텍스트가 그라운드 트루스로 간주되는 주석 답변과 얼마나 일치하는지를 측정합니다. 그라운드 트루스와 검색된 컨텍스트를 기반으로 계산하며 0에서 1 사이의 값을 가지며, 값이 높을수록 성능이 좋습니다.\n",
    "\n",
    "2. **Context relevancy:** 이 지표는 질문과 컨텍스트를 바탕으로 검색된 컨텍스트의 관련성을 측정합니다. 값은 (0, 1) 범위에 있으며, 값이 높을수록 관련성이 높습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import KnowledgeBasesEvaluations\n",
    "\n",
    "from ragas.metrics import (\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    )\n",
    "\n",
    "metrics = [context_recall,\n",
    "           context_precision\n",
    "           ]\n",
    "\n",
    "MODEL_ID_EVAL = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "MODEL_ID_GEN = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "questions = [\n",
    "        \"Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.\",\n",
    "]\n",
    "ground_truths = [\n",
    "    \"The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\\\n",
    "- Cash generated from operating activities amounted to $710 million, which can be attributed to a $700 million profit and non-cash charges such as depreciation and amortization.\\\n",
    "- Cash outflow from investing activities totaled $240 million, with major expenditures being the acquisition of property, plant, and equipment ($200 million) and marketable securities ($60 million), partially offset by the sale of property, plant, and equipment ($40 million) and maturing marketable securities ($20 million).\\\n",
    "- Financing activities resulted in a cash inflow of $350 million, stemming from the issuance of common stock ($200 million) and long-term debt ($300 million), while common stock repurchases ($50 million) and long-term debt payments ($100 million) reduced the cash inflow. \\\n",
    "Overall, Octank Financial experienced a net cash enhancement of $120 million in 2019, bringing their total cash and cash equivalents to $210 million.\",\n",
    "]\n",
    "kb_evaluate_standard = KnowledgeBasesEvaluations(model_id_eval=MODEL_ID_EVAL, \n",
    "                        model_id_generation=MODEL_ID_GEN, \n",
    "                        metrics=metrics,\n",
    "                        questions=questions, \n",
    "                        ground_truth=ground_truths, \n",
    "                        KB_ID=kb_id_standard,\n",
    "                        )\n",
    "\n",
    "kb_evaluate_hierarchical = KnowledgeBasesEvaluations(model_id_eval=MODEL_ID_EVAL, \n",
    "                        model_id_generation=MODEL_ID_GEN, \n",
    "                        metrics=metrics,\n",
    "                        questions=questions, \n",
    "                        ground_truth=ground_truths, KB_ID=kb_id_hierarchical)\n",
    "\n",
    "kb_evaluate_semantic = KnowledgeBasesEvaluations(model_id_eval=MODEL_ID_EVAL, \n",
    "                        model_id_generation=MODEL_ID_GEN, \n",
    "                        metrics=metrics,\n",
    "                        questions=questions, \n",
    "                        ground_truth=ground_truths, KB_ID=kb_id_semantic)\n",
    "\n",
    "kb_evaluate_custom = KnowledgeBasesEvaluations(model_id_eval=MODEL_ID_EVAL, \n",
    "                        model_id_generation=MODEL_ID_GEN, \n",
    "                        metrics=metrics,\n",
    "                        questions=questions, \n",
    "                        ground_truth=ground_truths, KB_ID=kb_id_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_heirarchical = kb_evaluate_hierarchical.evaluate()\n",
    "results_standard = kb_evaluate_standard.evaluate()\n",
    "results_semantic = kb_evaluate_semantic.evaluate()\n",
    "results_custoom = kb_evaluate_custom.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 800\n",
    "print(\"Fixed Chunking Evaluation for synthetic 10K report\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"Average context_recall: \", results_standard[\"context_recall\"].mean())\n",
    "print(\"Average context_relevancy: \", results_standard[\"context_precision\"].mean(), \"\\n\")\n",
    "\n",
    "print(\"Hierarchical Chunking Evaluation for synthetic 10K report\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"Average context_recall: \", results_heirarchical[\"context_recall\"].mean())\n",
    "print(\"Average context_relevancy: \", results_heirarchical[\"context_precision\"].mean(), \"\\n\")\n",
    "\n",
    "print(\"Semantic Chunking Evaluation for synthetic 10K report\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"Average context_recall: \", results_semantic[\"context_recall\"].mean())\n",
    "print(\"Average context_relevancy: \", results_semantic[\"context_precision\"].mean(), \"\\n\")\n",
    "\n",
    "print(\"Custom Chunking Evaluation for synthetic 10K report\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"Average context_recall: \", results_custoom[\"context_recall\"].mean())\n",
    "print(\"Average context_relevancy: \", results_custoom[\"context_precision\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===============================Knowledge base with fixed chunking==============================\\n\")\n",
    "knowledge_base_standard.delete_kb(delete_s3_bucket=True, delete_iam_roles_and_policies=True)\n",
    "print(\"===============================Knowledge base with hierarchical chunking==============================\\n\")\n",
    "knowledge_base_hierarchical.delete_kb(delete_s3_bucket=False,delete_iam_roles_and_policies=True)\n",
    "print(\"===============================Knowledge base with semantic chunking==============================\\n\")\n",
    "knowledge_base_semantic.delete_kb(delete_s3_bucket=False,delete_iam_roles_and_policies=True)\n",
    "print(\"===============================Knowledge base with custom chunking==============================\\n\")\n",
    "knowledge_base_custom.delete_kb(delete_s3_bucket=True,delete_iam_roles_and_policies=True, delete_lambda_function = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "bedrock_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}